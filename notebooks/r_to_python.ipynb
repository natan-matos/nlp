{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html  \n",
    "import csv\n",
    "import requests\n",
    "from time import sleep\n",
    "import re\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time as t\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] page_no_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: page_no_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36'}\n",
    "links_with_text = []\n",
    "final_city_links =[]\n",
    "info_scraped = {}\n",
    "\n",
    "#scraps all urls on the page\n",
    "def parse_url(url) :\n",
    "\tresponse=requests.get(url,headers=headers)\n",
    "\tsoup=BeautifulSoup(response.content,'lxml')\n",
    "\tt.sleep(3)\n",
    "\n",
    "\tfor a in soup.find_all('a', href=True, class_ = 'css-166la90'): \n",
    "    \t\tif a.text: \n",
    "        \t\tlinks_with_text.append(a['href'])\n",
    "\n",
    "#save only business URL\n",
    "def clean_urls(links_with_text):\n",
    "\tfor link in links_with_text:\n",
    "\t\tif (link[0:5] ==\"/biz/\"):\n",
    "\t\t\tinfo_scraped['URL'] = \"https://www.yelp.com\"+link\n",
    "\t\t\tfinal_city_links.append(info_scraped['URL'])\n",
    "\tprint(final_city_links)\t\t\n",
    "\tdf = pd.DataFrame({'URL':final_city_links})\n",
    "\treturn(df)\n",
    "\t\t\t\t\t\t\n",
    "#main function takes in list of page numbers as input and scraps it\t\t\n",
    "if __name__==\"__main__\":\n",
    "\targparser = argparse.ArgumentParser()\n",
    "\targparser.add_argument('page_no_file')\n",
    "\targparser.parse_args()\n",
    "\tfilename= sys.argv[1]\n",
    "\tpage_no = np.loadtxt(filename, delimiter=',')\n",
    "\tfor m in page_no:\n",
    "\t\tyelp_url  = \"https://www.yelp.com/search?cflt=restaurants&find_loc=Chicago&start=%s\"%(m)\n",
    "\t\tprint(m)\n",
    "\t\tscraped_data = parse_url(yelp_url)\n",
    "\tfinal_links = clean_urls(links_with_text)\n",
    "\tfinal_links.to_csv(\"url_yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (1431811388.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    tags = text.find_all('a', href = re.compile('/c/'))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "def res_scraper(url):\n",
    "\tdriver = webdriver.Firefox(options=fireFoxOptions)\n",
    "\tdriver.get(url)\n",
    "\t\n",
    "\tt.sleep(1)\n",
    "\tpage = driver.page_source\n",
    "\tsoup = BeautifulSoup(page, 'lxml')\n",
    "\tsoup2 = BeautifulSoup(page, 'html.parser')\n",
    "\tfinal_data = []\n",
    "\n",
    "\t# retrieve the total page number\n",
    "\tinfo_scraped = {}\n",
    "\tfinal_tag = ''\n",
    "\tfinal_address = ''\n",
    "\n",
    "\tinfo_scraped['restaurant_name'] = None\n",
    "\tinfo_scraped['restaurant_url'] = url\n",
    "\tinfo_scraped['restaurant_tag'] = None\n",
    "\tinfo_scraped['restaurant_neighborhood'] = None\n",
    "\tinfo_scraped['restaurant_address'] = None\n",
    "\tinfo_scraped['ratings'] = None\n",
    "\tinfo_scraped['review_number'] = None\n",
    "\tinfo_scraped['price'] = None\n",
    "\n",
    "\tall = soup.find('div', {'class': \"main-content-wrap main-content-wrap--full\"})\n",
    "\tspecial_divs = soup2.find_all('div',{'class':'main-content-wrap'})\n",
    "\t# retrieve tags and append to one string\n",
    "\ttry:\n",
    "\t\tfor text in special_divs:\n",
    "    \t\ttags = text.find_all('a', href = re.compile('/c/'))\n",
    "\t\t\n",
    "\t\tfor tag in tags:\n",
    "\t\t\tfinal_tag += tag.text + ','\n",
    "\t\t\tinfo_scraped['restaurant_tag'] = final_tag\n",
    "      \t\t\n",
    "\texcept:\n",
    "\t\tprint (None)\n",
    "\t\t\n",
    "    # retrieve restaurant name\n",
    "\ttry:\n",
    "\t\tinfo_scraped['restaurant_name'] = all.find('h1').text\n",
    "\texcept:\n",
    "\t\tprint(None)\n",
    "\n",
    "    # retrieve neighborhood on yelp, which now is CT_ID_10\n",
    "\ttry:\n",
    "\t\tfor text in special_divs:\n",
    "    \t\t\tneighbor = text.find_all('p', {'class': 'css-8yg8ez'})\n",
    "\t\tinfo_scraped['restaurant_neighborhood'] = neighbor[0].text\n",
    "\texcept:\n",
    "\t\tprint(None)\n",
    "\n",
    "    # retrieve address and append road, city, zip code to one string\n",
    "\ttry:\n",
    "\t\taddresses = all.find('address').find('p').find_all('span',{'class': 'raw__373c0__3rcx7'})\n",
    "\t\taddresses2 = all.find('address').find('p',{'class':'css-znumc2'}).find_all('span',{'class': 'raw__373c0__3rcx7'})\n",
    "\t\t\n",
    "\t\tfor address in addresses:\n",
    "            \t\tfinal_address += address.text + ','\n",
    "\t\tfor address in addresses2:\n",
    "            \t\tfinal_address += address.text + ','\n",
    "\t\tinfo_scraped['restaurant_address'] = final_address\n",
    "\texcept:\n",
    "\t\tprint(None)\n",
    "\n",
    "    # retrieve the average rating of each restaurant\n",
    "\ttry:\n",
    "\t\tinfo_scraped['ratings'] = all.find('div', {'aria-label': re.compile(' star rating')})['aria-label']\n",
    "\texcept:\n",
    "\t\tprint(None)\n",
    "\n",
    "    # retrieve total review numbers\n",
    "\ttry:\n",
    "\t\treview_number = all.find('span', {'class': 'css-bq71j2'}).text\n",
    "\t\treview_number = [int(i) for i in review_number.split() if i.isdigit()][0]\n",
    "\t\tinfo_scraped['review_number'] = review_number\n",
    "\texcept:\n",
    "\t\tprint(None)\n",
    "\n",
    "    # retrieve price category listed on YELP\n",
    "\ttry:\n",
    "\t\tprice_data = driver.find_element_by_xpath('/html/body/div[2]/div[3]/yelp-react-root/div/div[2]/div[1]/div[1]/div/div/span[2]/span').text\n",
    "\t\tif price_data[0] == '$':\n",
    "\t\t\tinfo_scraped['price'] = price_data\n",
    "\t\telse:\n",
    "\t\t\tinfo_scraped['price'] = ''\n",
    "\texcept:\n",
    "\t\tprint(None)\n",
    "\n",
    "\tfinal_data.append(info_scraped)\n",
    "\n",
    "\tdf = pd.DataFrame(final_data)\n",
    "\tdf.index += 1\n",
    "\tdriver.quit()\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "iteration_from = 60\n",
    "iteration_end = 61\n",
    "#iteration_end = len(urls)\n",
    "review_data = []\n",
    "\n",
    "# set driver to headless mode\n",
    "fireFoxOptions = webdriver.FirefoxOptions()\n",
    "fireFoxOptions.set_headless()\n",
    "\n",
    "for i in range(iteration_from, iteration_end):\n",
    "\tprint(str(datetime.now()) + \" \"+ str(i) + \" restaurant out of \" + str(len(urls)))\n",
    "\titem = urls[i] + '?sort_by=date_desc'\n",
    "\tresreview = res_scraper(item)\n",
    "\treview_data.append(resreview)\n",
    "\treview_all = pd.concat(review_data)\n",
    "    \n",
    "# encoding is utf-8-sig\n",
    "review_all.to_csv(\"Res_info60-61.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using random delay for time.sleep()\n",
    "delays = [7, 4, 6, 2, 10, 19]\n",
    "delay = np.random.choice(delays)\n",
    "\n",
    "def get_review(url, res_name, res_address):\n",
    "    binary = FirefoxBinary('/usr/bin/firefox')\n",
    "    opts = webdriver.FirefoxOptions()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(firefox_binary=binary, firefox_options=opts )\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(delay)\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    review_num = 0\n",
    "    final_data = []\n",
    "    num_page = 1\n",
    "    info_scraped = {}\n",
    "    info_scraped['reviewer_name'] = None\n",
    "    #info_scraped['reviewer_stat'] = None\n",
    "    info_scraped['reviewer_friends'] = None\n",
    "    info_scraped['reviewer_reviews'] = None\n",
    "    info_scraped['reviewer_photos'] = None\n",
    "    info_scraped['ratings'] = None\n",
    "    info_scraped['comment'] = None\n",
    "    info_scraped['review_date'] = None\n",
    "    info_scraped['reviewer_origin'] = None\n",
    "    info_scraped['reviewer_profile'] = None\n",
    "    # retrieve the total page number, if there is no information about this, it means the reviews have less than one full page, set page number to 1.\n",
    "    try:\n",
    "        total_page = driver.find_element_by_xpath('/html/body/div[2]/div[3]/yelp-react-root/div/div[3]/div/div/div[2]/div/div[1]/div[2]/section[2]/div[2]/div/div[4]/div[2]/span').text\n",
    "        print(total_page)\n",
    "        totalpage = [int(s) for s in total_page.split() if s.isdigit()]\n",
    "\n",
    "        num_page = totalpage[-1]\n",
    "        print(num_page)\n",
    "        #num_page = 1\n",
    "\n",
    "    except:\n",
    "        print(None)\n",
    "\n",
    "    # iterate through all pages\n",
    "    \n",
    "    print(url)\n",
    "    for page_np in range(num_page):\n",
    "        print('[{}] {} scraped page out of {}'.format(datetime.now(), page_np, num_page))\n",
    "        time.sleep(2)\n",
    "        page = driver.page_source\n",
    "        #soup = BeautifulSoup(page, 'lxml')\n",
    "        soup2 = BeautifulSoup(page, 'lxml')\n",
    "        \n",
    "        # retrieve all data on the site\n",
    "        all = soup.find_all('div', {'class': \"main-content-wrap main-content-wrap--full\"})\n",
    " \n",
    "        #special_all_stat = soup2.find_all('div',{'class': \" margin-t0-5__373c0__1VMSL border-color--default__373c0__3-ifU\"})\n",
    "        special_all_reviews = soup2.find_all('div',{'class': \"review__373c0__13kpL border-color--default__373c0__3-ifU\"})\n",
    "        review_num += len(special_all_reviews)\n",
    "\n",
    "        for i in range(len(special_all_reviews)):\n",
    "            info_scraped = {}\n",
    "            default = 'https://www.yelp.com'\n",
    "            stat = ''\n",
    "            origin = ''\n",
    "            # retrieve reviewer name\n",
    "            try:\n",
    "                special_user = special_all_reviews[i].find('div',{'class': \"user-passport-info border-color--default__373c0__3-ifU\"})\n",
    "                \n",
    "                info_scraped['reviewer_name'] = special_user.find('a').text\n",
    "                #print(info_scraped['reviewer_name'])\n",
    "            except:\n",
    "                print(None)\n",
    "\n",
    "            # retrieve reviewer statistic, like number of friends, number of reviews, elite or not.\n",
    "            try:\n",
    "                \n",
    "                for j in special_all_reviews[i].find_all('span', {'class': 'css-1dgkz3l'}) :\n",
    "                    stat += j.text\n",
    "                    stat += \" \"\n",
    "                #print(stat)\n",
    "                info_scraped['reviewer_friends'] = stat.split()[0]\n",
    "                info_scraped['reviewer_reviews'] = stat.split()[1]\n",
    "                info_scraped['reviewer_photos'] = stat.split()[2]\n",
    "            except:\n",
    "\n",
    "                print(None)\n",
    "\n",
    "            # retrieve the rating of this review\n",
    "            try:\n",
    "                \n",
    "                info_scraped['ratings'] = special_all_reviews[i].find('div', {\"aria-label\": re.compile('star rating')})[\"aria-label\"].split()[0]\n",
    "                #print(info_scraped['ratings'])\n",
    "            except:\n",
    "                print(None)\n",
    "\n",
    "            # retrieve the comment text the reviewer left\n",
    "            try:\n",
    "                \n",
    "                info_scraped['comment'] = special_all_reviews[i].find('p', {'class': 'comment__373c0__1M-px css-n6i4z7'}).find('span', {'class': 'raw__373c0__3rcx7'}).text\n",
    "               \n",
    "                #print(info_scraped['comment'])\n",
    "            except:\n",
    "\n",
    "                print(None)\n",
    "\n",
    "            # retrieve the date of review\n",
    "            try:\n",
    "                info_scraped['review_date'] = datetime.strptime(special_all_reviews[i].find('span', {\n",
    "                    'class': 'css-e81eai'}).text,'%m/%d/%Y').date()\n",
    "\n",
    "                #print(info_scraped['review_date'])\n",
    "            except:\n",
    "                print(None)\n",
    "\n",
    "            # retrieve origin of the reviewer and append them to one string\n",
    "            try:\n",
    "                origin = special_all_reviews[i].find('span',{'class':'css-n6i4z7'}).text\n",
    "                 \n",
    "                info_scraped['reviewer_origin'] = origin\n",
    "                \n",
    "            except:\n",
    "                print(None)\n",
    "\n",
    "            # retrieve profile website of each reviewer, prepared to retrieve the his history of ratting record\n",
    "            try:\n",
    "                info_scraped['reviewer_profile'] = default + special_all_reviews[i].find('a', {'class': 'css-166la90'}).attrs['href']\n",
    "               \n",
    "            except:\n",
    "                print(None)\n",
    "            print(\"********************************************************************************\")\n",
    "            \n",
    "            final_data.append(info_scraped)\n",
    "            \n",
    "        #find no of clickable buttons\n",
    "        clickable_button = soup.find_all('div',{'class': \"pagination-link-container__373c0__1mmdE border-color--default__373c0__3-ifU\"})\n",
    "        clicking_links = len(clickable_button)+2\n",
    "        click_link = str(clicking_links)\n",
    "        \n",
    "        # click the next button to go to next page\n",
    "            \n",
    "        if page_np == num_page-1:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            driver.find_element_by_xpath(\n",
    "                '//*[@id=\"wrap\"]/div[3]/yelp-react-root/div/div[3]/div/div/div[2]/div/div[1]/div[2]/section[2]/div[2]/div/div[4]/div[1]/div/div['+click_link+']/span/a/span').click()\n",
    "\n",
    "    address = res_address.strip()\n",
    "    restaurant_name = [res_name] * review_num\n",
    "    address = [address] * review_num\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(final_data)\n",
    "\n",
    "    df['restaurant name'] = pd.Series(restaurant_name)\n",
    "    df['address'] = pd.Series(address)\n",
    "    df.index += 1\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "iteration_from = 10\n",
    "iteration_end = 20\n",
    "review_data = []\n",
    "\n",
    "for i in range(iteration_from, iteration_end):\n",
    "    print(str(i) + \" restaurant out of \" + str(len(urls)))\n",
    "    item = urls[i]\n",
    "    name = res_name[i]\n",
    "    address = res_add[i]\n",
    "    resreview = get_review(item, name, address)\n",
    "    review_data.append(resreview)\n",
    "    review_all = pd.concat(review_data)\n",
    "    review_all.to_csv(\"Reviews\"+str(iteration_from)+\"-\"+str(iteration_end)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "\n",
    "boston_res_info = pd.read_csv(\"combined_reviews_info13001.csv\", encoding='utf-8-sig')\n",
    "rev_urls = boston_res_info['reviewer_profile'].tolist()\n",
    "\n",
    "delays = [1,2,3]\n",
    "delay = np.random.choice(delays)\n",
    "\n",
    "#keep changing proxy list based on https://sslproxies.org/\n",
    "def get_review(url):\n",
    "\n",
    "    myProxy = ['167.172.123.221','129.21.253.179','159.89.221.73']\n",
    "    proxys = np.random.choice(myProxy)\n",
    "    proxy = Proxy({\n",
    "    'proxyType': ProxyType.MANUAL,\n",
    "    'httpProxy': proxys,\n",
    "    'ftpProxy': proxys,\n",
    "    'sslProxy': proxys,\n",
    "    })\n",
    "\n",
    "    opts = webdriver.FirefoxOptions()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    \n",
    "    driver = webdriver.Firefox(firefox_options=opts,proxy=proxy,executable_path='C:/Users/user/Downloads/geckodriver.exe')\n",
    "    driver.get(url)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    print(url)\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "    info_scraped = {}\n",
    "    final_data = []\n",
    "\n",
    "    # retrieve the total page number\n",
    "    try:\n",
    "        info_scraped['Name'] = soup.find('h1').text\n",
    "    except:\n",
    "        print(None)\n",
    "\n",
    "    try:\n",
    "        info_scraped['Origin'] = soup.find('h3', {'class': 'user-location alternate'}).text\n",
    "    except:\n",
    "        print(None)\n",
    "\n",
    "    time.sleep(delay)\n",
    "    # retrieve the number of rating history recorde\n",
    "    try:\n",
    "        rating_list = soup.find_all('td', {'class': 'histogram_count'})\n",
    "        info_scraped['5_star'] = rating_list[0].text\n",
    "        info_scraped['4_star'] = rating_list[1].text\n",
    "        info_scraped['3_star'] = rating_list[2].text\n",
    "        info_scraped['2_star'] = rating_list[3].text\n",
    "        info_scraped['1_star'] = rating_list[4].text\n",
    "    except:\n",
    "        print(None)\n",
    "\n",
    "    final_data.append(info_scraped)\n",
    "    df = pd.DataFrame(final_data)\n",
    "    df.index += 1\n",
    "\n",
    "    driver.quit()\n",
    "    return df\n",
    "\n",
    "\n",
    "iteration_from = 500\n",
    "iteration_end = 1000\n",
    "review_data = []\n",
    "for i in range(iteration_from, iteration_end):\n",
    "    print(str(datetime.now()) + ' ' + str(i) + \" reviewer out of \" + str(len(rev_urls)))\n",
    "    item = rev_urls[i]\n",
    "\n",
    "    reviewer_info = get_review(item)\n",
    "    review_data.append(reviewer_info)\n",
    "    review_star = pd.concat(review_data)\n",
    "    review_star.to_csv(\"Rating distribution-13001-\"+str(iteration_from)+\"-\"+str(iteration_end)+\".csv\", encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
